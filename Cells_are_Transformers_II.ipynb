{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cells are Transformers II",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrGHEtiPd56MneVjNBLau2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikcl/CellTransformer/blob/wip-no-repeat/Cells_are_Transformers_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g305-z3wlXTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b49e57-1b94-4585-df56-33b42f548f55"
      },
      "source": [
        "!pip install linear-attention-transformer\n",
        "!pip install einops"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting linear-attention-transformer\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/aa/014fa1cf088fe07584fc0968acd5a840855b05b94dd791bcf47886c3201b/linear_attention_transformer-0.18.0-py3-none-any.whl\n",
            "Collecting product-key-memory>=0.1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/31/3b/c1f8977e4b04f047acc7b23c7424d1e2e624ed7031e699a2ac2287af4c1f/product_key_memory-0.1.10.tar.gz\n",
            "Collecting einops\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting axial-positional-embedding\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from linear-attention-transformer) (1.8.1+cu101)\n",
            "Collecting linformer>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/5f/10f3ebeb144982a0170b5ec9dcf80e6ae82a6e6e9afddd21c9bf985e62c5/linformer-0.2.1-py3-none-any.whl\n",
            "Collecting local-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->linear-attention-transformer) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->linear-attention-transformer) (1.19.5)\n",
            "Building wheels for collected packages: product-key-memory, axial-positional-embedding\n",
            "  Building wheel for product-key-memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-cp37-none-any.whl size=3072 sha256=410f61f963accb670ac6f4bc5dd3fd9773c118029f0a1965aefa8129d3dcc8b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/e0/3b/fd3111a4fac652ed014ccfd4757754f006132723985e229419\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=790ac6ab32f03ff475a1dbc8fbeee55a7d56f6b5132eedffc8a7d186aaa6d43d\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built product-key-memory axial-positional-embedding\n",
            "Installing collected packages: product-key-memory, einops, axial-positional-embedding, linformer, local-attention, linear-attention-transformer\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 linear-attention-transformer-0.18.0 linformer-0.2.1 local-attention-1.2.2 product-key-memory-0.1.10\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awl8-4fPlAZ8"
      },
      "source": [
        "import os\n",
        "import base64\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "import tqdm\n",
        "\n",
        "import io\n",
        "import PIL.Image, PIL.ImageDraw\n",
        "import base64\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "import matplotlib.pylab as plt\n",
        "import glob\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from linear_attention_transformer import LinearAttentionTransformerLM\n",
        "from linear_attention_transformer.autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "DRIVE_PATH = '/content/gdrive/My Drive/'\n",
        "\n",
        "\n",
        "YOUR_FOLDER_PATH = 'KCL CS/Year 4/PRJ/models/'\n",
        "\n",
        "GOOGLE_DRIVE_FOLDER = DRIVE_PATH + YOUR_FOLDER_PATH\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90lPXBTtlaph"
      },
      "source": [
        "#@title Cellular Automata Parameters\n",
        "CHANNEL_N = 4        # Number of CA state channels\n",
        "TARGET_PADDING = 16   # Number of pixels used to pad the target image border\n",
        "TARGET_SIZE = 40\n",
        "\n",
        "# üëÅ, ü¶é  'ü¶éüòÄüí•üëÅüê†ü¶ãüêûüï∏ü•®üéÑ'\n",
        "TARGET_EMOJI = \"ü¶é\" #@param {type:\"string\"}\n",
        "\n",
        "EXPERIMENT_TYPE = \"Growing\" #@param [\"Growing\"] # \"Persistent\", \"Regenerating\"\n",
        "EXPERIMENT_MAP = {\"Growing\":0, \"Persistent\":1, \"Regenerating\":2}\n",
        "EXPERIMENT_N =  0 #EXPERIMENT_MAP[EXPERIMENT_TYPE]\n",
        "\n",
        "USE_PATTERN_POOL = [0, 1, 1][EXPERIMENT_N]\n",
        "DAMAGE_N = [0, 0, 3][EXPERIMENT_N]  # Number of patterns to damage in a batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTMuZlP6m4Z6"
      },
      "source": [
        "#@title Transformer Token Details\n",
        "TOKEN_OFFSET = 2\n",
        "NUM_TOKENS = 257 + TOKEN_OFFSET\n",
        "ENC_SEQ_LEN = 9 * CHANNEL_N\n",
        "DEC_SEQ_LEN = CHANNEL_N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCiVj1WmnAE"
      },
      "source": [
        "#@title Tranformer Hyperparameters \n",
        "\n",
        "EPOCHS = 300 #int(1e3)\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "# GENERATE_EVERY  = 100   # For visual results during training. . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPB73jkn1cB"
      },
      "source": [
        "#@title Helpers \n",
        "\n",
        "def to_rgba(x):\n",
        "  return x[:,:4,:,:]\n",
        "\n",
        "def to_rgb(x):\n",
        "  return x[:,:3,:,:]\n",
        "\n",
        "def make_seed(size, n=1):\n",
        "  x = np.zeros([n, size, size, CHANNEL_N], np.float32)\n",
        "  x[:, size//2, size//2, 3:] = 1.0\n",
        "  return x\n",
        "\n",
        "def plot_loss(loss_log):\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  plt.title('Loss history (log10)')\n",
        "  plt.plot(np.log10(loss_log), '.', alpha=0.1)\n",
        "  plt.show()\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in [np.float32, np.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fup4z40WpClo"
      },
      "source": [
        "#@title Load image utils. \n",
        "\n",
        "# Returns h,w,c image where c=RGBA\n",
        "def load_image(url, max_size=TARGET_SIZE):\n",
        "  r = requests.get(url)\n",
        "  print('R',r)\n",
        "  img = PIL.Image.open(io.BytesIO(r.content))\n",
        "  img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
        "  # img = np.float32(img)\n",
        "  # return img\n",
        "\n",
        "  # ASSUME THIERS\n",
        "  img = np.float32(img)/255.0\n",
        "  # premultiply normalised RGB by Alpha\n",
        "  img[..., :3] *= img[..., 3:]\n",
        "  img = img*255.0\n",
        "  return img\n",
        "\n",
        "def load_emoji(emoji):\n",
        "  code = hex(ord(emoji))[2:].lower()\n",
        "  url = 'https://github.com/googlefonts/noto-emoji/raw/main/png/128/emoji_u%s.png'%code\n",
        "  return load_image(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rKmByqYpcE6"
      },
      "source": [
        "#@title To Device\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "device = torch.device(dev)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpswGT_xsezf"
      },
      "source": [
        "#@title Load Emoji\n",
        "target_img = load_emoji(TARGET_EMOJI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7C4qZRsGnt"
      },
      "source": [
        "p = TARGET_PADDING\n",
        "target_img_tensor = torch.from_numpy(target_img)\n",
        "\n",
        "# (72,72,4)\n",
        "pad_target = torch.nn.functional.pad(target_img_tensor, pad=(0,0,p,p,p,p), mode='constant', value=0)\n",
        "h, w = pad_target.shape[:2]\n",
        "\n",
        "# (CHANNEL_N, 72, 72)\n",
        "seed = np.zeros([CHANNEL_N, h, w], np.float32)\n",
        "seed[3:, h//2, w//2] = 255\n",
        "\n",
        "pad_target = pad_target.permute(2, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM2KOzGus8Hv"
      },
      "source": [
        "pad_target = pad_target.to(device)\n",
        "print(pad_target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP3syDMHy-Jv"
      },
      "source": [
        "#@title Masking Utils\n",
        "IMAGE_BATCH_SIZE = 1 #number of images to concurrently train on. \n",
        "LIVE_MIN = 0\n",
        "\n",
        "# Applied on default/non-tokenized values\n",
        "def get_living_mask(x, rate=LIVE_MIN, k=3):\n",
        "  alpha = x[:, 3:4, :, :]\n",
        "  p = 1 if k > 1 else 0\n",
        "  return torch.nn.functional.max_pool2d(input=alpha, kernel_size=k, stride=1, padding=p) > rate\n",
        "\n",
        "# 3d tensor as boolean. rate = 0 before, k =3\n",
        "alive_target_mask = get_living_mask(pad_target[None,...]/255.0, rate=0.1, k=1)\n",
        "\n",
        "loss_target_mask = get_living_mask(pad_target[None,...]/255.0, rate=0.1, k=1)\n",
        "loss_target = ((pad_target/255.0) * loss_target_mask.float().bool().long())\n",
        "\n",
        "def cleaned_x_long(x, bool_mask):\n",
        "  return  (x * bool_mask.float().bool().long()).long()\n",
        "\n",
        "def t_loss_f(x, clean=False):\n",
        "    x = x[:,:,:,:]\n",
        "\n",
        "    x = x if not clean else cleaned_x_long(x,loss_target_mask)\n",
        "    return np.mean(torch.square((x/255.0)-loss_target).cpu().detach().numpy(), axis=1)\n",
        "\n",
        "\n",
        "def loss_score(loss):\n",
        "    return np.log10(np.mean(loss))\n",
        "\n",
        "def plot_loss_diff(loss):\n",
        "  img_loss = loss.reshape(loss.shape[1],loss.shape[2])  # TODO may be wrong indexes.\n",
        "\n",
        "  plt.imshow(zoom(img_loss, 1), vmin=0, vmax=1, cmap='jet') #'gray_r'\n",
        "  plt.colorbar()\n",
        "\n",
        "\n",
        "# TODO REMOVE FUNC\n",
        "def loss_f(x, clean=False):\n",
        "    x = x[:,:,:,:]\n",
        "\n",
        "    # ls = nn.MSELoss()\n",
        "    # err = ls(x[0]/255.0, pad_target.long()/255.0).cpu().detach().numpy()\n",
        "    return np.mean(torch.square((x/255.0)-(pad_target/255.0)).cpu().detach().numpy(), axis=1)\n",
        "\n",
        "\n",
        "def get_dilatied_mask(x):\n",
        "  # Returns x dilated mask as FloatType \n",
        "  output = x.sum(-3) > 0\n",
        "  output = output.type(torch.FloatTensor)\n",
        "  kernel = torch.tensor([[1,1,1],[1,1,1],[1,1,1]]).type(torch.FloatTensor)\n",
        "  dilated = torch.nn.functional.conv2d(output[None,...], kernel[None, None, ...].expand(1, -1, -1, -1), groups=1, padding=1)\n",
        "  return dilated\n",
        "\n",
        "def get_living_target(x):\n",
        "  living_mask = get_living_mask(x)\n",
        "\n",
        "  # print('SUM OF 1s', torch.sum(torch.flatten(living_mask)))\n",
        "  living_target_mask = (living_mask & alive_target_mask)\n",
        "  living_target = pad_target[None,...] * living_target_mask.float()\n",
        "  living_target = living_target[0]\n",
        "  return living_target\n",
        "\n",
        "def mask_target(mask):\n",
        "  '''\n",
        "    mask - tensor: 2D - Float\n",
        "\n",
        "    returns - tensor: 4D - T\n",
        "  '''\n",
        "  # returns 4d tensor. \n",
        "  target_batch = pad_target[None, ...] * mask\n",
        "  return target_batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1fglUUQteEd"
      },
      "source": [
        "#@title Define Model\n",
        "def tokenize(x):\n",
        "  return (x + 2).long()\n",
        "\n",
        "def detokenize(x):\n",
        "  return (x - 2).long()\n",
        "\n",
        "def percieve(x):\n",
        "  # x - 4d tensor - N, C, H, W\n",
        "  _,_,h,w = x.shape\n",
        "  padded_x = torch.nn.functional.pad(x, pad=(1,1,1,1), mode='constant', value=0)\n",
        "  stacked_image = torch.cat(\n",
        "      [padded_x[:,:,i:i+h, j:j+w]  for j in range(3) for i in range(3)]\n",
        "      ,1\n",
        "  )\n",
        "  return stacked_image\n",
        "\n",
        "\n",
        "# instantiate model\n",
        "enc = LinearAttentionTransformerLM(\n",
        "    num_tokens = NUM_TOKENS,\n",
        "    dim = 1024,\n",
        "    heads = 8,\n",
        "    depth = 6,\n",
        "    max_seq_len = ENC_SEQ_LEN,\n",
        "    # one_kv_head = True,\n",
        "    use_axial_pos_emb=False,\n",
        "    return_embeddings = True\n",
        ").to(device)\n",
        "\n",
        "dec = LinearAttentionTransformerLM(\n",
        "    num_tokens = NUM_TOKENS,\n",
        "    dim = 1024,\n",
        "    heads = 8,\n",
        "    depth = 6,\n",
        "    causal = True,\n",
        "    blindspot_size = 1,\n",
        "    max_seq_len = DEC_SEQ_LEN,\n",
        "    # one_kv_head = True,\n",
        "    use_axial_pos_emb=False,\n",
        "    receives_context = True\n",
        ").to(device)\n",
        "\n",
        "dec = AutoregressiveWrapper(dec)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5lGSTAXnsnL"
      },
      "source": [
        "#@title Load Saved Model Instead\n",
        "LOAD_MODEL = False\n",
        "enc_path = GOOGLE_DRIVE_FOLDER + 'enc-first.pth'\n",
        "dec_path = GOOGLE_DRIVE_FOLDER + 'dec-first.pth'\n",
        "if (LOAD_MODEL):\n",
        "  enc = torch.load(enc_path)\n",
        "  # enc.eval()\n",
        "  dec = torch.load(dec_path)\n",
        "  # dec.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRfYEZakngND"
      },
      "source": [
        "optim = torch.optim.Adam([*enc.parameters(), *dec.parameters()], lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIz10RkPL14v"
      },
      "source": [
        "#@title Dataset Generation Helpers. \n",
        "SENTENCES = int(alive_target_mask.shape[2] * alive_target_mask.shape[3])\n",
        "\n",
        "class CellTraining(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.src = X\n",
        "        self.tgt = Y\n",
        "        assert X.shape[0] == Y.shape[0], 'Dataset X and Y not of same length'\n",
        "\n",
        "        b = X.shape[0]\n",
        "        self.SRC_MASK = torch.ones(b, ENC_SEQ_LEN).bool()\n",
        "        self.TGT_MASK = torch.ones(b, DEC_SEQ_LEN).bool()\n",
        "\n",
        "        self.length = b\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src[idx]\n",
        "        tgt = self.tgt[idx]\n",
        "        src_mask = self.SRC_MASK[idx]\n",
        "        tgt_mask = self.TGT_MASK[idx] \n",
        "        return src.to(device), tgt.to(device), src_mask.to(device), tgt_mask.to(device)\n",
        "\n",
        "\n",
        "class CellEvaluating(Dataset):\n",
        "    def __init__(self, X):\n",
        "        self.src = X\n",
        "        b = X.shape[0]\n",
        "        self.SRC_MASK = torch.ones(b, ENC_SEQ_LEN).bool()\n",
        "        self.length = b\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src[idx]\n",
        "        src_mask = self.SRC_MASK[idx]\n",
        "        return src.to(device), src_mask.to(device)\n",
        "\n",
        "\n",
        "\n",
        "def get_start_tokens(number_of_sentences):\n",
        "    return torch.ones((number_of_sentences, 1)).long().to(device)\n",
        "\n",
        "def image_to_sentences(x):\n",
        "  '''\n",
        "    ## maps a 3D tensor of shape (c, h, w) to a 2D tensor of (h*w, c)\n",
        "    these are known as sentences or 'patches' of size one\n",
        "  '''\n",
        "  return rearrange(x,'c h w -> (h w) c')\n",
        "\n",
        "\n",
        "def sentences_to_image(x, height, width):\n",
        "  '''\n",
        "    ## maps a 2D tesnor of shape (h*w, c) to image/3D tensor shape of (c, h, w)\n",
        "    inverse of image_to_sentence\n",
        "    height: int - image dimension\n",
        "    width: int - image dimension\n",
        "  '''\n",
        "  return rearrange(x, '(h w) (p1 p2 c) -> c (h p1) (w p2)', p1 =1, p2 =1, h=height, w=width)\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "def visualize_batched(x, c=4):\n",
        "  x = x[:,:c,:,:].cpu().numpy()\n",
        "  vis1 = np.hstack(np.transpose(x, (0,2,3,1)))\n",
        "  plt.imshow(vis1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_X_values(x, mask=None):\n",
        "  # X values\n",
        "  percieved_images = percieve(x)\n",
        "  X_img = percieved_images[0]\n",
        "\n",
        "  # TODO - optional mask here? \n",
        "  X_img = X_img if mask==None else (X_img[None,...] * mask)[0]\n",
        "\n",
        "  X_seq_un_clean = image_to_sentences(X_img)\n",
        "  valid_instances = (torch.tensor(X_seq_un_clean).bool().float().sum(dim=-1) != 0).nonzero().flatten().long()\n",
        "\n",
        "  X_seq_cleaned = X_seq_un_clean[valid_instances]\n",
        "\n",
        "  X_seq = tokenize(X_seq_cleaned)\n",
        "  return valid_instances, X_seq\n",
        "\n",
        "\n",
        "# X - 4d tensor - maybe move this\n",
        "def get_input_values(x):\n",
        "  step_output_float_mask = get_dilatied_mask(x).to(device)\n",
        "\n",
        "  valid_instances, X_seq = get_X_values(x)\n",
        "\n",
        "  return step_output_float_mask, valid_instances, X_seq\n",
        "\n",
        "def get_update_values(x, step_float, seen_float):\n",
        "  training_float = (step_float.bool() ^ seen_float.bool()).float()\n",
        "  seen_float = (training_float.bool() | seen_float.bool()).float()\n",
        "\n",
        "  # TODO - use training float to extract valid instances\n",
        "  valid_instances, X_seq = get_X_values(x, training_float)\n",
        "\n",
        "  return training_float, valid_instances, X_seq, seen_float\n",
        "\n",
        "def get_X0():\n",
        "  x0 = np.repeat(seed[None,...], IMAGE_BATCH_SIZE, 0)\n",
        "  x0 = torch.Tensor(x0).to(device)\n",
        "  return x0\n",
        "\n",
        "def create_training_dataset():\n",
        "  x0 = get_X0()\n",
        "  x = x0\n",
        "  prev_N = None\n",
        "  seen_float = (get_living_mask(x).float() * 0).to(device);\n",
        "\n",
        "  steps = 0\n",
        "\n",
        "  x_values = []\n",
        "  y_values = []\n",
        "\n",
        "  while (prev_N != 0):\n",
        "\n",
        "    step_output_float_mask, valid_instances, X_seq = get_input_values(x)\n",
        "\n",
        "    u_step_output_float_mask, u_valid_instances, u_X_seq, seen_float = get_update_values(x, step_output_float_mask, seen_float)\n",
        "    N = len(u_valid_instances)\n",
        "    prev_N = N\n",
        "    if (N != 0):\n",
        "      steps+=1\n",
        "      training_float_mask, valid_instances, X_seq = u_step_output_float_mask, u_valid_instances, u_X_seq\n",
        "    else:\n",
        "      training_float_mask = step_output_float_mask\n",
        "\n",
        "    y_target = mask_target(training_float_mask.bool().float())[0]\n",
        "\n",
        "    # Prefix\n",
        "    prefix = get_start_tokens(X_seq.shape[0])\n",
        "\n",
        "    # Y valyes from target\n",
        "    y_seq_un_cleaned = image_to_sentences(y_target)\n",
        "    y_seq_cleaned = y_seq_un_cleaned[valid_instances]\n",
        "    y_seq = tokenize(y_seq_cleaned)\n",
        "    Y_seq = torch.cat([prefix, y_seq], 1)\n",
        "\n",
        "    # Add X_seq, Y_seq to a dataset\n",
        "    x_values.append(X_seq)\n",
        "    y_values.append(Y_seq)\n",
        "\n",
        "    x = mask_target(step_output_float_mask.bool().float()).to(device)\n",
        "\n",
        "  X_seqs = torch.cat(x_values, 0)\n",
        "  Y_seqs = torch.cat(y_values, 0)\n",
        "  training_dataset = CellTraining(X_seqs, Y_seqs)\n",
        "  return training_dataset, steps\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbe5tHcSF4J"
      },
      "source": [
        "training_dataset, EVAL_STEPS = create_training_dataset()\n",
        "print('LEN DATASET', len(training_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exA36QjGVKIE"
      },
      "source": [
        "#@title Generate Method { vertical-output: true}\n",
        "SENTENCE_BATCH = 32\n",
        "\n",
        "def generate(encoder, decoder, x, EVAL_STEPS, disable=False):\n",
        "  encoder.eval(), decoder.eval()\n",
        "\n",
        "  for step in tqdm.auto.tqdm(range(EVAL_STEPS), mininterval=1., desc='evaluation step', disable=disable):\n",
        "    output_seq = torch.zeros(SENTENCES, CHANNEL_N).long().to(device)\n",
        "\n",
        "    step_output_float_mask, update_instances, X_seq = get_input_values(x)\n",
        "\n",
        "    update_instances = update_instances\n",
        "\n",
        "    dataset = CellEvaluating(X_seq)\n",
        "    eval_data_loader = DataLoader(dataset=dataset, batch_size=SENTENCE_BATCH, shuffle=False)\n",
        "    \n",
        "    generated= torch.zeros(X_seq.shape[0], CHANNEL_N).long().to(device)\n",
        "\n",
        "    for batch_i , (src, src_mask) in enumerate(eval_data_loader):\n",
        "      start_tokens = (torch.ones((src.shape[0], 1)) * 1).long().to(device)\n",
        "\n",
        "      context = encoder(src)\n",
        "      \n",
        "      sample = decoder.generate(start_tokens, DEC_SEQ_LEN , context = context)\n",
        "      predicted_cells = sample[:, - CHANNEL_N: ]\n",
        "      s = batch_i*SENTENCE_BATCH\n",
        "      \n",
        "      locations = torch.arange(s,s+predicted_cells.shape[0])\n",
        "\n",
        "      generated[locations] = predicted_cells\n",
        "\n",
        "    output_seq[update_instances] = detokenize(generated)\n",
        "    output_img = sentences_to_image(output_seq, x.shape[2], x.shape[3])\n",
        "    \n",
        "    x = output_img[None,...]\n",
        "\n",
        "  return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVWObOcGJPv"
      },
      "source": [
        "#@title Un-Ordered Training!!!!! { vertical-output: true}\n",
        "\n",
        "# training\n",
        "log_loss = []\n",
        "\n",
        "TRAIN_ON=True \n",
        "EVAL_EVERY=50\n",
        "VISUALISE_EVERY = 50\n",
        "\n",
        "next_token_loss = [['EPOCH', 'STEP', 'LOSS']]\n",
        "epoch_losses = []\n",
        "epoch_mean_losses = []\n",
        "\n",
        "training_dataset, EVAL_STEPS = create_training_dataset()\n",
        "print('LEN DATASET', len(training_dataset))\n",
        "data_loader = DataLoader(dataset=training_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "TRAINING_STEPS = EPOCHS\n",
        "\n",
        "for i in tqdm.tqdm(range(1,TRAINING_STEPS + 1), mininterval=10., desc='training epochs'):\n",
        "    if (TRAIN_ON):\n",
        "      losses = []    \n",
        "      enc.train(), dec.train()\n",
        "      for batch_i , (src, tgt, src_mask, tgt_mask) in enumerate(data_loader):\n",
        "        context = enc(src, input_mask = src_mask)\n",
        "        loss = dec(tgt, context = context, input_mask = tgt_mask, context_mask = src_mask, return_loss = True)\n",
        "        loss.backward()\n",
        "        loss_value = loss.item()\n",
        "        next_token_loss.append([i, batch_i, loss_value])\n",
        "        losses.append(loss_value)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "      epoch_mean_losses.append(sum(losses)/len(losses)  )\n",
        "      epoch_losses.append(losses)\n",
        "\n",
        "    # Generate:\n",
        "    if (EVAL_STEPS > 0 and i % EVAL_EVERY == 0):\n",
        "      x = get_X0()\n",
        "\n",
        "      x = generate(enc, dec, x, EVAL_STEPS)\n",
        "\n",
        "      un_clean_loss = t_loss_f(x)\n",
        "      lo_n = loss_score(un_clean_loss)\n",
        "      log_loss.append(lo_n)\n",
        "\n",
        "      if (lo_n < -4):\n",
        "        print('Early threshold Exit..')\n",
        "        break\n",
        "\n",
        "    # Visualise progress\n",
        "    if (i % VISUALISE_EVERY == 0):\n",
        "      visualize_batched(x.detach().clone())\n",
        "      plt.figure()\n",
        "      for ep, l in enumerate(epoch_losses):\n",
        "        plt.plot(l, label=f'EPOCH-{i+ep}')\n",
        "\n",
        "      plt.xlabel(\"Step\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.title(f\"epoch-{i}\")\n",
        "      # plt.legend()\n",
        "      plt.show()\n",
        "      epoch_losses = []\n",
        "\n",
        "      plt.figure()\n",
        "      plt.plot(epoch_mean_losses)\n",
        "\n",
        "      plt.xlabel(\"epoch\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.title(f\"Training Loses Over Epochs\")\n",
        "      plt.show()\n",
        "\n",
        "      if (len(log_loss) > 0):\n",
        "        plt.figure()\n",
        "        plt.plot(log_loss, label=f'EPOCH-{i+ep}')\n",
        "\n",
        "        plt.xlabel(f\"every - {EVAL_EVERY-1} training steps\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Log Loss\")\n",
        "        plt.show()\n",
        "\n",
        "      \n",
        "    # TODO - produced x - calculate l2 loss and add to log loss - need alignment first"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZGLN7Z9E4K"
      },
      "source": [
        "#@title Save Models { vertical-output: true}\n",
        "import time\n",
        "\n",
        "SAVE_MODEL= True\n",
        "if (SAVE_MODEL):\n",
        "  ts = str(time.time()).split('.')[0]\n",
        "  current_t = ts\n",
        "\n",
        "  ENC_FILE = f'./enc-{str(current_t)}.pth'\n",
        "  DEC_FILE = f'./dec-{str(current_t)}.pth'\n",
        "\n",
        "  torch.save(enc, ENC_FILE)\n",
        "  drive_path = f'/content/gdrive/My Drive/KCL CS/Year 4/PRJ/models/{ENC_FILE}'\n",
        "  shutil.copyfile(ENC_FILE, drive_path)\n",
        "\n",
        "  torch.save(dec, DEC_FILE)\n",
        "  drive_path = f'/content/gdrive/My Drive/KCL CS/Year 4/PRJ/models/{DEC_FILE}'\n",
        "  shutil.copyfile(DEC_FILE, drive_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmZ8Q85s4DW_"
      },
      "source": [
        "# HELPER BLOCK? - Visualises last x again\n",
        "\n",
        "cleaned_x =  cleaned_x_long(x, loss_target_mask) \n",
        "\n",
        "visualize_batched(x)\n",
        "visualize_batched(pad_target[None,...].long())\n",
        "visualize_batched(cleaned_x)\n",
        "\n",
        "to_diff = cleaned_x #cleaned_x\n",
        "\n",
        "loss = t_loss_f(to_diff)\n",
        "plot_loss_diff(loss)\n",
        "\n",
        "print('TLOSS', loss_score(t_loss_f(x)) )\n",
        "print('TLOSS CLEAN',loss_score(t_loss_f(x, clean=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ambcN0ju88pT"
      },
      "source": [
        "#@title Multiple Image Generation { vertical-output: true}\n",
        "\n",
        "NUMBER_TO_GENERATE = 20\n",
        "\n",
        "best_gen, best_score = None, float('inf')\n",
        "\n",
        "GEN_FROM_TARGET = False\n",
        "\n",
        "for _ in tqdm.auto.tqdm(range(NUMBER_TO_GENERATE), mininterval=1., desc='generating images'):\n",
        "  GEN_FROM = cleaned_x_long(pad_target,loss_target_mask) if GEN_FROM_TARGET else get_X0()\n",
        "\n",
        "  x = generate(enc, dec, GEN_FROM, EVAL_STEPS, disable=True)\n",
        "  score = loss_score(t_loss_f(x, clean=True))\n",
        "  if score  < best_score:\n",
        "    best_score = score\n",
        "    best_gen = x\n",
        "  print(score)\n",
        "\n",
        "print(best_score)\n",
        "visualize_batched(cleaned_x_long(best_gen, loss_target_mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es1ge73hdX_7"
      },
      "source": [
        "visualize_batched(best_gen)\n",
        "plot_loss_diff(t_loss_f(best_gen, clean=True))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aF0firhfb4M"
      },
      "source": [
        "#@title Persistance Tests { vertical-output: true}\n",
        "\n",
        "START_FROM_TARGET = False  # False will start from best generated, True from target position. \n",
        "CLEANED_BEST_GEN = True\n",
        "\n",
        "ROUNDS = 3 \n",
        "SCORE_EVERY = 10\n",
        "# TOTAL STEPS = ROUNDS * SCORE_EVERY\n",
        "\n",
        "c_scores = []\n",
        "uc_scores = []\n",
        "\n",
        "# use best generated or use target state? \n",
        "bg = cleaned_x_long(best_gen, loss_target_mask) if CLEANED_BEST_GEN else best_gen\n",
        "x = cleaned_x_long(pad_target,loss_target_mask) if START_FROM_TARGET else bg\n",
        "\n",
        "for _ in tqdm.auto.tqdm(range(ROUNDS), mininterval=1., desc='persistent experiementing...'):\n",
        "  x = generate(enc, dec, x, SCORE_EVERY, disable=True)\n",
        "  unmasked_score = loss_score(t_loss_f(x, clean=False))\n",
        "  uc_scores.append(unmasked_score)\n",
        "\n",
        "  masked_score = loss_score(t_loss_f(x, clean=True))\n",
        "  c_scores.append(masked_score)\n",
        "\n",
        "print(c_scores)\n",
        "print(uc_scores)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbM85fuij1GZ"
      },
      "source": [
        "print(c_scores)\n",
        "print(uc_scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuSAvoY2oQyG"
      },
      "source": [
        "visualize_batched(x)\n",
        "visualize_batched(cleaned_x_long(x, loss_target_mask))\n",
        "\n",
        "\n",
        "loss = t_loss_f(x)\n",
        "plot_loss_diff(loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}